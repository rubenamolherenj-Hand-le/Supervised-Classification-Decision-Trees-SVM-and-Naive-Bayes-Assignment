{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "jSlOm_yhroLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer\n",
        "\n",
        "\n",
        "Information Gain is a metric used to train decision trees by measuring the reduction in \"entropy\" or randomness after a dataset is split on a specific attribute. It helps the algorithm decide which feature should be placed at a node to best separate the data into distinct classes.\n",
        "\n",
        "- The process follows these steps:\n",
        "\n",
        "1. Calculate Entropy: Measure the impurity of the current dataset.\n",
        "\n",
        "2. Split Data: Temporarily split the data based on a feature.\n",
        "\n",
        "3. Calculate Weighted Entropy: Find the average entropy of the resulting branches.\n",
        "\n",
        "4. Subtract: Information Gain = (Original Entropy) - (Weighted Entropy of the split). The attribute with the highest Information Gain is selected as the splitting node."
      ],
      "metadata": {
        "id": "jyqcCnspr0pB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer\n",
        "\n",
        " - Here is the difference between Gini Impurity and Entropy presented in simple lines (points):\n",
        "\n",
        "1. **Concept**: Gini Impurity measures the probability of mislabeling a randomly chosen element, while Entropy measures the amount of information or disorder (uncertainty) in the system.\n",
        "\n",
        "2. **Formula Base**: Gini Impurity uses the sum of squared probabilities, whereas Entropy uses logarithmic calculations (base 2).\n",
        "\n",
        "3. **Computational Cost**: Gini Impurity is computationally faster because squaring numbers is easier for a processor than calculating logarithms, which Entropy requires.\n",
        "\n",
        "4. **Value Range**: Gini Impurity ranges from 0 to 0.5 (for binary classification), whereas Entropy ranges from 0 to 1.\n",
        "\n",
        "5. **Maximum Impurity**: A Gini value of 0.5 indicates maximum impurity (randomness), while for Entropy, a value of 1.0 indicates maximum uncertainty.\n",
        "\n",
        "6. **Sensitivity**: Entropy is slightly more sensitive to changes in the probability distribution and tends to penalize impurities more heavily than Gini.\n",
        "\n",
        "7. **Tree Structure**: Gini Impurity tends to isolate the most frequent class in its own branch, whereas Entropy tends to create slightly more balanced trees.\n",
        "\n",
        "8. **Algorithms**: Gini is the default criterion for the CART algorithm (used in standard Decision Trees), while Entropy is used in ID3 and C4.5 algorithms.\n",
        "\n",
        "9. **Practicality**: In 95% of real-world use cases, both yield very similar model performance, so Gini is often preferred simply for its speed."
      ],
      "metadata": {
        "id": "bLodq0gYsDxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer\n",
        "\n",
        " **Pre-pruning**, also known as \"early stopping,\" involves halting the growth of a decision tree before it perfectly classifies the training set. This is done to prevent overfitting.\n",
        "\n",
        "- Common pre-pruning techniques include:\n",
        "\n",
        "1. Setting a maximum depth for the tree.\n",
        "\n",
        "2. Setting a minimum number of samples required to split a node.\n",
        "\n",
        "3. Defining a threshold for the minimum Information Gain required to continue splitting."
      ],
      "metadata": {
        "id": "wCHIQyaNuC8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)?\n",
        "Answer\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Classifier with Gini Impurity [cite: 22]\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print Feature Importances [cite: 22]\n",
        "feature_importances = pd.Series(clf.feature_importances_, index=iris.feature_names)\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importances.sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUZNoARwt7x7",
        "outputId": "0433a1f8-b6ce-4376-e428-8e280a2c00dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machines (SVM)**"
      ],
      "metadata": {
        "id": "l9J1kiarvdYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression. Its primary goal is to find the optimal hyperplane in an N-dimensional space that maximizes the \"margin\" between data points of different classes. The points closest to the hyperplane that influence its position are called Support Vectors."
      ],
      "metadata": {
        "id": "Jrxhho3lvuf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer\n",
        "\n",
        "The Kernel Trick is a mathematical technique that allows SVMs to solve non-linear problems by projecting the data into a higher-dimensional space where a linear separator (hyperplane) can be found. Instead of performing expensive transformations, it uses \"kernel functions\" (like RBF or Polynomial) to calculate the inner products of the data in that high-dimensional space directly."
      ],
      "metadata": {
        "id": "pr4ojjQ-vyuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies ?\n",
        "Answer\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear Kernel [cite: 33]\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# RBF Kernel [cite: 33]\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IBQAVWJvVN9",
        "outputId": "3d195639-3ae7-4fe0-fd1b-8cc60cd931a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9815\n",
            "RBF Kernel Accuracy: 0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Naive Bayes**"
      ],
      "metadata": {
        "id": "FHgR0GnJwhi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer\n",
        "\n",
        "Naive Bayes is a classification algorithm based on Bayes' Theorem. It is called \"Naive\" because it makes the strong (and often unrealistic) assumption that all features are independent of each other given the class label. For example, in a fruit classifier, a fruit may be considered an apple if it is red, round, and 3 inches in diameter; Naïve Bayes assumes each of these features contributes independently to the probability, regardless of any correlations."
      ],
      "metadata": {
        "id": "F37jk5BOwxRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial\n",
        "Naïve Bayes, and Bernoulli Naïve Bayes ?\n",
        "\n",
        "Answer\n",
        "\n",
        "These variants are chosen based on the distribution of the features:\n",
        "\n",
        "- Gaussian NB: Used when features follow a normal (Gaussian) distribution (e.g., height, weight).\n",
        "\n",
        "- Multinomial NB: Used for discrete counts (e.g., word counts in text classification).\n",
        "\n",
        "- Bernoulli NB: Used for binary/boolean features (e.g., whether a word occurs in a document or not)."
      ],
      "metadata": {
        "id": "qDFGEJSpxFrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy ?\n",
        "\n",
        "Answer\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset [cite: 45]\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train GaussianNB [cite: 45]\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate Accuracy [cite: 44]\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(f\"Gaussian Naïve Bayes Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phptpfWnwTC5",
        "outputId": "ce9f4913-2a9e-4387-ee7c-debd9c8c9e3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes Accuracy: 0.9415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fQ0holzcxwOn"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}